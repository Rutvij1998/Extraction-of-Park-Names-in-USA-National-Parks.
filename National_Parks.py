# -*- coding: utf-8 -*-
"""national_parks.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11exccE1DTUTa2wi_zdHweD3xOND1lydt
"""

# Importing all the Necessary Libraries
from requests import get
from bs4 import BeautifulSoup
import csv
import re
from itertools import zip_longest

# Declaring all the variables and Empty Lists to store the Extracted Values
name_url_list = []
url = []
address_link=[]
AddressLinesPara = []
AllstateParkLinks=[]
Line1=[]
Line2=[]
Line3=[]
desc_main=[]
category=[]
locality=[]
desc_data=[]
AllStateParkNames=[]
Address_text=[]
AddRegion=[]
Phone=[]
AddLocality=[]
PostCode = []
Line1=[]
Line2=[]
Line3=[]
url_link = []
# The line which i missed to write while copying from rough code to actual code.
url = "https://www.nps.gov/index.htm"
page_html = get(url)
soup = BeautifulSoup(page_html.text, 'html.parser')
# First We will Iterate through the Home Page to Get the names of all states and the link to their webpage
for ul in soup.findAll('ul', class_='dropdown-menu SearchBar-keywordSearch'):
  for link in ul.findAll('a',href=True):
            #print(link.text) 
    temp_link = link['href']
    actual_link = 'https://www.nps.gov' + temp_link
    #print(actual_link)
    name_url_list.append(link.text)
    name_url_list.append(actual_link)
    url_link.append(actual_link)

# Then We need to iterate through each State's page to extract the name of the park, link to the page of park, Description, Category
for i in url_link:
      WebPage = get(i)
      WebPageSoup = BeautifulSoup(WebPage.text, 'html.parser')
      for divison in WebPageSoup.findAll('div',class_='col-md-9 col-sm-9 col-xs-12 table-cell list_left'):
        for ParkName in divison.findAll('a',href = True):
          AllStateParkNames.append(ParkName.text)
          temp_link = ParkName['href']
          actual_link = 'https://www.nps.gov' + temp_link
          AllstateParkLinks.append(actual_link)
        for cat in divison.find('h2'):
          if(len(cat) is not None):
            category.append(cat)
          else:
            category.append('\n')
        for loc in divison.findAll('span',itemprop='addressLocality'):
          locality.append(loc)
        for Description in divison.find('p'):
          desc_data.append(Description)

# Then we will Iterate through each of the Park's Page to get the contact information of the Page.
for i in AllstateParkLinks:
  Parks = get(i)
  ParkSoup = BeautifulSoup(Parks.text,'html.parser')
  if(ParkSoup.find_all('p',class_='adr')):
    for address in ParkSoup.find_all('p',class_='adr'):
      if(address.find('span',class_='street-address')):
        Line = address.find('span',class_='street-address')
        Address_text.append(Line.get_text().strip())
      else:
        Address_text.append('')
      if(address.find('span',itemprop = "addressLocality")):
        Locality = address.find('span',itemprop = "addressLocality")
        AddLocality.append(Locality.get_text())
      else:
        AddLocality.append('')
      if(address.find('span',itemprop = "addressRegion")):
        Region = address.find('span',itemprop = "addressRegion")
        AddRegion.append(Region.get_text())
      else:
        AddRegion.append('')
      if(address.find('span',itemprop = "postalCode")):
        PCode = address.find('span',itemprop = "postalCode")
        PostCode.append(PCode.get_text())
      else:
        PostCode.append('')
  for phone in ParkSoup.findAll('span',class_='tel'):
      Phone.append(phone.get_text())

# The address is of 3 lines for some parks, we need to seperate as they are now stored as one value.
for StrAddr in Address_text:
                  listAddress = re.split('\n',StrAddr)
                  if(len(listAddress) == 3):
                    Line1.append(listAddress[0])
                    Line2.append(listAddress[1])
                    Line3.append(listAddress[2])
                  if(len(listAddress) == 2):
                    Line1.append(listAddress[0])
                    Line2.append(listAddress[1])
                    Line3.append('')
                  if(len(listAddress) == 1):
                    Line1.append(listAddress[0])
                    Line2.append('')
                    Line3.append('')
# Lastly we need to save the lists into a csv file.
d = [AllStateParkNames,category,desc_data,Line1,Line2,Line3,AddLocality,AddRegion,PostCode,Phone]
export_data = zip_longest(*d, fillvalue = '')
with open('National_Parks.csv', 'w', newline='') as myfile:
      wr = csv.writer(myfile)
      wr.writerow(("Park Name", "Category","Description","Street address","Line 2","Line 3","City","State","Zip Code","Phone Number"))
      wr.writerows(export_data)
myfile.close()
